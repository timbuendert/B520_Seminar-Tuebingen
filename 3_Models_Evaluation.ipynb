{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "#3_Models_Evaluation.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "_qmiAKsqYSfq",
        "cM8a43ZcbHUj",
        "JMxBYjZRbHUm",
        "8tZsky-KbHUm",
        "7rgiNy9_bHUo",
        "oUrhTQl7bHUp",
        "FvQTopCObHUp",
        "uU5PHAIN5TOT"
      ],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J0buZl-VbHUh"
      },
      "source": [
        "# Notebook 3: Model building and evaluation\n",
        "\n",
        "This notebook entails the building and evaluation of the machine learning models introduced in section 3.2 of the paper. Based on the tuned variants of the learning algorithms, their explainability is evaluated and compared.\n",
        "The results are presented in the paper in section 4.3.\n",
        "\n",
        "**Table of Contents**:\n",
        "\n",
        "0. [Technical setup](#setup)\n",
        "1. [Load data and define functions](#setup)\n",
        "2. [Build, evaluate and explain machine learning models](#models)\n",
        "    1. [Random Forest](#rf)\n",
        "    2. [Support Vector Machine](#svm)\n",
        "    3. [XGBoost](#xgb)\n",
        "    4. [LSTM](#lstm)\n",
        "3. [Performance comparison](#comp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_qmiAKsqYSfq"
      },
      "source": [
        "# 3.0 Technical setup <a id=\"setup\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ovHnnC9Lh3NV"
      },
      "source": [
        "!pip install gensim==4.0.1\n",
        "!pip install shap\n",
        "!pip install lime\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S8BXlqKHbHUh",
        "outputId": "457a6b54-7e64-4369-97b9-3164bb65f849"
      },
      "source": [
        "# import modules\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.dates as mdates\n",
        "from collections import Counter\n",
        "import string\n",
        "from copy import deepcopy\n",
        "import seaborn as sns\n",
        "from tqdm.notebook import tqdm\n",
        "import statistics\n",
        "import warnings\n",
        "import random\n",
        "from scipy import stats\n",
        "\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, RepeatedKFold, cross_val_score, cross_validate, RandomizedSearchCV, GridSearchCV, RepeatedStratifiedKFold\n",
        "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score, confusion_matrix, make_scorer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.dummy import DummyClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "from gensim.models import FastText\n",
        "\n",
        "from lime import lime_text\n",
        "from lime.lime_text import LimeTextExplainer\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import *\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from keras.initializers import Constant\n",
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "from xgboost import XGBClassifier"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
            "  warnings.warn(msg)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t6c_-j0ybHUi"
      },
      "source": [
        "# define functions for saving and loading pickled objects\n",
        "def save_pickle(objectname, picklename):\n",
        "    pickle_out = open(picklename,\"wb\")\n",
        "    pickle.dump(objectname, pickle_out)\n",
        "    pickle_out.close()\n",
        "    print(picklename, 'successfully pickled.') \n",
        "    \n",
        "def load_pickle(picklename):\n",
        "    pickle_in = open(picklename,\"rb\")\n",
        "    return pickle.load(pickle_in)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cM8a43ZcbHUj"
      },
      "source": [
        "# 3.1 Load data and define functions <a id=\"load\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CkQ6VzjgbHUj",
        "outputId": "282eff09-ecc5-4be5-fe8a-e66b9c4ca3aa"
      },
      "source": [
        "text = load_pickle(\"/content/drive/MyDrive/Seminar/hateXplain_processed.pickle\")\n",
        "X = load_pickle(\"/content/drive/MyDrive/Seminar/data_corpus.pickle\")\n",
        "Y = load_pickle(\"/content/drive/MyDrive/Seminar/labels.pickle\")\n",
        "rationales = load_pickle(\"/content/drive/MyDrive/Seminar/rationales.pickle\")\n",
        "    \n",
        "# check for correct lengths\n",
        "print(X.shape, type(X))\n",
        "print(len(Y), type(Y))\n",
        "print(len(rationales), type(rationales))\n",
        "\n",
        "# load embeddings model\n",
        "text_model = FastText.load('/content/drive/MyDrive/Seminar/model1.bin')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(20147, 300) <class 'numpy.ndarray'>\n",
            "20147 <class 'pandas.core.series.Series'>\n",
            "20147 <class 'pandas.core.series.Series'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AfEK6-MAbHUi"
      },
      "source": [
        "# function to create a sentence vector based on a list of tokens with \n",
        "# the defined embeddings model\n",
        "def sentence_vector(sentence, d):\n",
        "    X = np.zeros([len(sentence), d])\n",
        "    for i in range(len(sentence)):\n",
        "        wv = text_model.wv[sentence[i]]\n",
        "        norm_wv = np.linalg.norm(wv)\n",
        "        with np.errstate(invalid='ignore'):\n",
        "            X[i] = wv/norm_wv\n",
        "            \n",
        "    with warnings.catch_warnings():\n",
        "        warnings.simplefilter(\"ignore\", category=RuntimeWarning)\n",
        "        sen_vector = np.nanmean(X, axis = 0)\n",
        "           \n",
        "    return sen_vector\n",
        "\n",
        "# function to evaluate the explainability of the respective model using LIME\n",
        "# the mean explainability measure among the n posts to be evaluated is returned\n",
        "def explain_clf(n, classifier = \"Not LSTM\"):\n",
        "    explainer = LimeTextExplainer(class_names=classes)\n",
        "    results = []\n",
        "\n",
        "    for i in tqdm(range(len(ind_rat[:n])), \"Explaining Predictions\"):\n",
        "        n_rat = sum(text[\"rationales_comb\"][ind_rat[i]]) #* 2\n",
        "        if classifier == \"LSTM\":\n",
        "          exp = explainer.explain_instance(\" \".join(text[\"tokens_processed\"][ind_rat[i]]), \n",
        "                                           predict_LSTM, \n",
        "                                           num_features = n_rat)\n",
        "          \n",
        "        else:\n",
        "          exp = explainer.explain_instance(\" \".join(text[\"tokens_processed\"][ind_rat[i]]), \n",
        "                                           predict_clf, \n",
        "                                           num_features = n_rat)\n",
        "\n",
        "        res = exp.as_list()\n",
        "        results.append(eval_explanation(res, i))\n",
        "\n",
        "    return statistics.mean(results)\n",
        "\n",
        "\n",
        "# function to predict the class probabilities based on texts and a trained clf\n",
        "# by retrieving the sentence_vector and using .predict_proba()\n",
        "def predict_clf(texts):\n",
        "    d = len(text_model.wv['x'])\n",
        "    text_data_pred = np.zeros([len(texts),len(text_model.wv['x'])])\n",
        "    for i in range(len(texts)):\n",
        "        tokens = texts[i].split()\n",
        "        text_data_pred[i, :] = sentence_vector(tokens, d)\n",
        "    text_data_pred = np.nan_to_num(text_data_pred)\n",
        "    pred = clf.predict_proba(text_data_pred)\n",
        "    return pred\n",
        "\n",
        "\n",
        "# function to predict the class probabilities based on texts and a trained LSTM\n",
        "# model by converting the texts into padded sequences ans using .predict()\n",
        "def predict_LSTM(texts):\n",
        "  x = tokenizer.texts_to_sequences(texts)\n",
        "  x = pad_sequences(x, sequence_length)\n",
        "  pred = LSTM_model.predict(x) # for one prediction of index 0 only: predict(X_padded[:1])\n",
        "  return np.c_[ pred, 1-pred ] \n",
        "\n",
        "\n",
        "# function to evaluate the explanations provided by LIME, specifically,\n",
        "# the relative relationship of correct identified rationales to overll correct\n",
        "# rationales is returned\n",
        "def eval_explanation(res_obj, ind):\n",
        "    rat = text[\"rationales_comb\"][ind_rat[ind]]\n",
        "    correct_rat = [i for i, x in enumerate(rat) if x == 1]\n",
        "    \n",
        "    tokens = text[\"tokens_processed\"][ind_rat[ind]]\n",
        "    try:\n",
        "      pred_rat = [tokens.index(res_obj[i][0]) for i in range(len(res_obj))]\n",
        "      correct_pred = [i for i in pred_rat if i in correct_rat]    \n",
        "      return len(correct_pred)/len(correct_rat)\n",
        "\n",
        "    except ValueError:\n",
        "      return 0\n",
        "\n",
        "# function to plot the confusion matrix of the cross-validation\n",
        "def plot_cm(cm):\n",
        "  df_cm = pd.DataFrame(np.around(cm/np.sum(cm), 2), range(len(set(Y))), range(len(set(Y))))\n",
        "  df_cm.index.name = 'True'\n",
        "  df_cm.columns.name = 'Predicted'\n",
        "  ax = plt.axes()\n",
        "  sns.set(font_scale=1.4) # for label size\n",
        "  sns.heatmap(df_cm, annot=True, annot_kws={\"size\": 10}, ax = ax, fmt='g')"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JMxBYjZRbHUm"
      },
      "source": [
        "# 3.2 Build, evaluate and explain machine learning models <a id=\"models\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i03CtHcObHUk"
      },
      "source": [
        "# collect indices which contain rationales & randomly shuffle them\n",
        "random.seed(0)\n",
        "ind_rat = [i for i in range(text.shape[0]) if len(text.rationales_comb[i]) > 2] \n",
        "random.shuffle(ind_rat)\n",
        "\n",
        "# define names of classes\n",
        "classes = [\"normal\", \"HateSpeech\"]\n",
        "\n",
        "# construct 5 shuffled and stratified folds for evaluating clssifier performance\n",
        "kfold = RepeatedKFold(n_splits=5, n_repeats=1, random_state=42)\n",
        "\n",
        "# function to return the name of object\n",
        "def namestr(obj):\n",
        "    return [name for name in globals() if globals()[name] is obj]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8tZsky-KbHUm"
      },
      "source": [
        "## 3.2.1 Random Forest <a id=\"rc\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NAUqfy9NbHUn",
        "scrolled": true
      },
      "source": [
        "# parameter tuning of a random forest classifier via grid search\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "parameters = {'max_depth':[50, 90, 150], \"n_estimators\":[50, 100, 130], \"min_samples_leaf\":[2, 10], \n",
        "              \"max_samples\":[0.3, 0.5], \"min_samples_split\":[10, 20], 'bootstrap':(True, False)}\n",
        "              \n",
        "rf = RandomForestClassifier()\n",
        "clf = GridSearchCV(rf, parameters)\n",
        "clf.fit(X, Y)\n",
        "\n",
        "clf.best_params_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "18T0nhGnbHUn"
      },
      "source": [
        "# evaluating the performance of the tuned random forest classifier by 5-fold \n",
        "# cross-validation\n",
        "\n",
        "rf_best = RandomForestClassifier(bootstrap=True, class_weight='balanced',criterion='gini', \n",
        "                                 max_depth=90, max_features='auto', max_samples=0.3,                                   \n",
        "                                 min_samples_leaf=2, min_samples_split=10, n_estimators=130,                                   \n",
        "                                 n_jobs=-1, random_state=42)\n",
        "\n",
        "acc = []\n",
        "rf_cm = np.zeros((2, 2))\n",
        "\n",
        "for train_ix, test_ix in kfold.split(X, Y):\n",
        "    train_X, test_X = X[train_ix], X[test_ix]\n",
        "    train_y, test_y = Y[train_ix], Y[test_ix]\n",
        "    clf_deep = deepcopy(rf_best)\n",
        "    clf_new = clf_deep.fit(train_X, train_y)\n",
        "    pred_new = clf_new.predict(test_X)\n",
        "\n",
        "    cm = confusion_matrix(test_y, pred_new)\n",
        "    rf_cm += cm\n",
        "    acc.append(accuracy_score(test_y, pred_new))\n",
        "\n",
        "print('Accuracy:', np.mean(acc))\n",
        "print(acc)\n",
        "\n",
        "plot_cm(rf_cm)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JK3Og5nNbHUn",
        "scrolled": true
      },
      "source": [
        "# explain the predictions of the tuned random forest classifier\n",
        "\n",
        "clf = RandomForestClassifier(bootstrap=True, class_weight='balanced',criterion='gini', \n",
        "                             max_depth=90, max_features='auto', max_samples=0.3,     \n",
        "                             min_samples_leaf=2, min_samples_split=10, n_estimators=130,   \n",
        "                             n_jobs=-1, random_state=42)\n",
        "clf = clf.fit(X, Y)\n",
        "explain_clf(500)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nd_S4anV1w7H"
      },
      "source": [
        "# visualization of LIME applied to the first post using random forest\n",
        "\n",
        "explainer = LimeTextExplainer(class_names=[\"Normal\", \"Hate Speech\"])\n",
        "exp = explainer.explain_instance(\" \".join(text[\"tokens_processed\"][ind_rat[0]]), new_predict, num_features=6)\n",
        "exp.show_in_notebook(text=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7rgiNy9_bHUo"
      },
      "source": [
        "## 3.2.2 SVM <a id=\"svm\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "coknpfVibHUo"
      },
      "source": [
        "# parameter tuning of a SVM classifier via grid search\n",
        "\n",
        "parameters = {'kernel':('linear', 'rbf'), 'C':[1, 10, 100]}\n",
        "              \n",
        "svm = SVC(max_iter=-1, probability=True, tol=0.001)\n",
        "clf = GridSearchCV(svm, parameters)\n",
        "clf.fit(X, Y)\n",
        "\n",
        "clf.best_params_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U-9dkSs6bHUp",
        "scrolled": true
      },
      "source": [
        "# evaluating the performance of the tuned SVM classifier by 5-fold cross-validation\n",
        "\n",
        "svc_best = SVC(C=100, kernel='rbf', max_iter=-1, random_state=42)\n",
        "\n",
        "acc = []\n",
        "svm_cm = np.zeros((2, 2))\n",
        "\n",
        "for train_ix, test_ix in kfold.split(X, Y):\n",
        "    train_X, test_X = X[train_ix], X[test_ix]\n",
        "    train_y, test_y = Y[train_ix], Y[test_ix]\n",
        "\n",
        "    clf_deep = deepcopy(svc_best)\n",
        "    clf_new = clf_deep.fit(train_X, train_y)\n",
        "    pred_new = clf_new.predict(test_X)\n",
        "\n",
        "    cm = confusion_matrix(test_y, pred_new)\n",
        "    svm_cm += cm\n",
        "\n",
        "    acc.append(accuracy_score(test_y, pred_new))\n",
        "\n",
        "print('Accuracy:', np.mean(acc))\n",
        "print(acc)\n",
        "\n",
        "plot_cm(svm_cm)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v8ydENRdbHUp"
      },
      "source": [
        "# explain the predictions of the tuned SVM classifier\n",
        "\n",
        "clf = SVC(C=100, kernel='rbf', max_iter=-1, random_state=42, probability=True)\n",
        "\n",
        "clf = clf.fit(X, Y)\n",
        "explain_clf(500)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oUrhTQl7bHUp"
      },
      "source": [
        "## 3.2.3 XGBoost <a id=\"xgb\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KjmIfxc9bHUp"
      },
      "source": [
        "# create own class due to adaption\n",
        "\n",
        "class MyXGBClassifier(XGBClassifier):\n",
        "    @property\n",
        "    def coef_(self):\n",
        "        return None"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V5xB-4i1bHUp"
      },
      "source": [
        "# parameter tuning of a XGBoost classifier via grid search\n",
        "\n",
        "parameters = {'learning_rate':[0.3, 0.6], 'max_depth':[100, 150], 'n_estimators':[100, 150, 300],\n",
        "              'reg_lambda':[1, 1.5], 'gamma':[0.2, 0.5], 'reg_alpha':[0.25, 0.5]}\n",
        "              \n",
        "xgb = MyXGBClassifier(booster = 'gbtree', objective= \"binary:logistic\", use_label_encoder=False,\n",
        "                      eval_metric = 'logloss')\n",
        "\n",
        "clf = GridSearchCV(xgb, parameters)\n",
        "clf.fit(X, Y)\n",
        "\n",
        "clf.best_params_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7P3jfLaObHUp",
        "scrolled": true
      },
      "source": [
        "# evaluating the performance of the XGBoost classifier with optimal\n",
        "# parameters evaluated by grid search\n",
        "\n",
        "xgb_best = MyXGBClassifier(learning_rate=0.3, booster = 'gbtree', objective= \"binary:logistic\", use_label_encoder=False, \n",
        "                           max_depth = 100, gamma = 0.2, reg_alpha = 0.25, reg_lambda = 1.5, eval_metric = 'logloss', \n",
        "                           n_estimators=300, random_state = 42, n_jobs = -1)\n",
        "                     \n",
        "acc = []\n",
        "xgb_cm = np.zeros((2, 2))\n",
        "\n",
        "for train_ix, test_ix in kfold.split(X, Y):\n",
        "    train_X, test_X = X[train_ix], X[test_ix]\n",
        "    train_y, test_y = Y[train_ix], Y[test_ix]\n",
        "    clf_deep = deepcopy(xgb_best)\n",
        "    clf_new = clf_deep.fit(train_X, train_y)\n",
        "    pred_new = clf_new.predict(test_X)\n",
        "    \n",
        "    cm = confusion_matrix(test_y, pred_new)\n",
        "    xgb_cm += cm\n",
        "    f1.append(f1_score(test_y, pred_new, average = 'binary', labels=np.unique(test_y)))\n",
        "    acc.append(accuracy_score(test_y, pred_new))\n",
        "\n",
        "print('Accuracy:', np.mean(acc))\n",
        "print(acc)\n",
        "\n",
        "plot_cm(xgb_cm)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FYRKPLxmbHUp"
      },
      "source": [
        "# explain the predictions of the tuned XGBoost classifier\n",
        "\n",
        "clf = MyXGBClassifier(learning_rate=0.3, booster = 'gbtree', objective= \"binary:logistic\", use_label_encoder=False, \n",
        "                           max_depth = 100, gamma = 0.2, reg_alpha = 0.25, \n",
        "                           reg_lambda = 1.5, eval_metric = 'logloss', n_estimators=300, random_state = 42, n_jobs = -1)\n",
        "\n",
        "clf = clf.fit(X, Y)\n",
        "\n",
        "explain_clf(500)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SvlHjPs50k47"
      },
      "source": [
        "# evaluating the randomness of LIME explanations by repeating the explanation of the same 50\n",
        "# posts 30 times and computing the variance of the explainability scores \n",
        "\n",
        "exp_scores = []\n",
        "for i in range(30):\n",
        "  exp_scores.append(explain_clf(50))\n",
        "print(np.var(exp_scores))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FvQTopCObHUp"
      },
      "source": [
        "## 3.2.4 LSTM <a id=\"lstm\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "azXRmtgT3VJV"
      },
      "source": [
        "# setup LSTM architecture to return a model which can be trained subsequently\n",
        "def get_LSTM():\n",
        "  model = Sequential()\n",
        "  model.add(Embedding(num_words, # add embedding layer based on the trained embeddings model\n",
        "                      embedding_dim,\n",
        "                      embeddings_initializer=Constant(embedding_matrix),\n",
        "                      input_length=sequence_length,\n",
        "                      trainable=False))\n",
        "  model.add(SpatialDropout1D(0.2)) # include dropout layer to counteract overfitting\n",
        "  model.add(Bidirectional(LSTM(300, return_sequences=True)))\n",
        "  model.add(Bidirectional(LSTM(150)))\n",
        "  model.add(Dropout(0.25))\n",
        "  model.add(Dense(units=1, activation='sigmoid')) \n",
        "  model.compile(loss = 'binary_crossentropy', optimizer='adam', metrics = ['acc'])\n",
        "  print(model.summary())\n",
        "  return model\n",
        "\n",
        "# define function to print the history of the training of the LSTM network with\n",
        "# regard to accuracy and loss\n",
        "def show_plt():\n",
        "  plt.plot(history.history['acc'])\n",
        "  plt.plot(history.history['val_acc'])\n",
        "  plt.title('model accuracy')\n",
        "  plt.ylabel('accuracy')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(['train', 'validation'], loc='upper left')\n",
        "  plt.show()\n",
        "\n",
        "  plt.plot(history.history['loss'])\n",
        "  plt.plot(history.history['val_loss'])\n",
        "  plt.title('model loss')\n",
        "  plt.ylabel('loss')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(['train', 'validation'], loc='upper left')\n",
        "  plt.show()"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ITe9TL_f4DFQ"
      },
      "source": [
        "# determine lengths of post of post corpus\n",
        "length_posts = [len(text[\"tokens_processed\"][i]) for i in range(text.shape[0])]\n",
        "\n",
        "batch_size = 64\n",
        "sequence_length = max(length_posts) # define maximum length of post for subsequent padding \n",
        "embedding_dim = 300 # as defined in the embeddings model\n",
        "\n",
        "texts = text['tokens_processed']\n",
        "\n",
        "# tokenize the processed tokens such that they can be easily padded and an index\n",
        "# with regard to their position can be returned\n",
        "tokenizer = Tokenizer(split=' ', oov_token='<unw>', filters=' ')\n",
        "tokenizer.fit_on_texts(texts.values)\n",
        "word_index = tokenizer.word_index\n",
        "X_token = tokenizer.texts_to_sequences(texts.values)\n",
        "X_padded = pad_sequences(X_token, sequence_length) # pad the sequences so they are all the same length (sequence_length)\n",
        "\n",
        "# define number of words as the true number plus one to account for padding\n",
        "num_words = len(word_index) + 1\n",
        "\n",
        "# initialize the embedding matrix with zeros\n",
        "embedding_matrix = np.zeros((num_words, embedding_dim))\n",
        "\n",
        "# for each word in the tokenizer, find the vector in the embeddings model\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = text_model.wv[word]\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector # add vector to matrix\n",
        "    else:\n",
        "        # if word is not in model (which should not be the case):\n",
        "        # assign random vector\n",
        "        embedding_matrix[i] = np.random.randn(embedding_dim)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6n_ACE4gpY0j"
      },
      "source": [
        "# evaluating the performance of the LSTM network by 5-fold cross-validation\n",
        "\n",
        "acc = []\n",
        "lstm_cm = np.zeros((2, 2))\n",
        "\n",
        "for train_ix, test_ix in kfold.split(X_padded, Y):    \n",
        "  train_X, test_X = X_padded[train_ix], X_padded[test_ix]\n",
        "  train_y, test_y = Y[train_ix], Y[test_ix]\n",
        "\n",
        "  LSTM_model = get_LSTM()\n",
        "  history = LSTM_model.fit(train_X, train_y, epochs=20, batch_size=batch_size, verbose=1, validation_split=0.3, callbacks=[EarlyStopping(monitor='acc', min_delta=0.01, patience=2, restore_best_weights = True)])\n",
        "  #show_plt()\n",
        "  pred_new = LSTM_model.predict(test_X)\n",
        "  \n",
        "  cm = confusion_matrix(test_y, np.around(pred_new))\n",
        "  lstm_cm += cm\n",
        "  f1.append(f1_score(test_y, np.around(pred_new), average = 'binary', labels=np.unique(test_y)))\n",
        "  acc.append(accuracy_score(test_y, np.around(pred_new)))\n",
        "\n",
        "\n",
        "print('Accuracy:', np.mean(acc))\n",
        "print(acc)\n",
        "\n",
        "plot_cm(lstm_cm)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hr7SxQl4Zwxn"
      },
      "source": [
        "# explain the predictions of the LSTM network\n",
        "\n",
        "LSTM_model = get_LSTM()\n",
        "history = LSTM_model.fit(X_padded, Y, epochs=20, batch_size=batch_size, verbose=1, validation_split=0.3, callbacks=[EarlyStopping(monitor='acc', min_delta=0.01, patience=2, restore_best_weights = True)])\n",
        "\n",
        "explain_clf(500, classifier = \"LSTM\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uU5PHAIN5TOT"
      },
      "source": [
        "# 3.3 Performance comparison <a id=\"comp\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5gQf-SQN5i39"
      },
      "source": [
        "# store accuracy - and explainability score of respective classifiers\n",
        "\n",
        "rf_acc, rf_exp = 0.6742, 0.6511\n",
        "svm_acc, svm_exp = 0.7109, 0.6668\n",
        "xgboost_acc, xgboost_exp = 0.6928, 0.6617\n",
        "lstm_acc, lstm_exp = 0.7123, 0.6931\n",
        "\n",
        "x = [rf_acc, svm_acc, xgboost_acc, lstm_acc]\n",
        "y = [rf_exp, svm_exp, xgboost_exp, lstm_exp]"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iTwJ-PZJKRI5"
      },
      "source": [
        "# quantify relationship between accuracy and explainability for the analyzed models\n",
        "\n",
        "slope, intercept, r_value, p_value, std_err = stats.linregress(x,y)\n",
        "print(p_value)\n",
        "print(slope)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KtcZM6xB7uV5"
      },
      "source": [
        "# plot predictive performance against explainability performance\n",
        "\n",
        "coef = np.polyfit(x,y,1)\n",
        "poly1d_fn = np.poly1d(coef) \n",
        "\n",
        "colors=[\"red\", \"blue\", \"green\", \"black\", \"orange\"]\n",
        "\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111)\n",
        "\n",
        "for i in range(len(x)):\n",
        "    ax.scatter(x[i], y[i], color=colors[i])\n",
        "\n",
        "ax.grid(False)\n",
        "ax.set(facecolor = \"white\")\n",
        "\n",
        "plt.legend([\"Random forest\", \"SVM\", \"XGBoost\", \"LSTM\"], loc=2, bbox_to_anchor=(1.05, 1), borderaxespad=0., fontsize=11)\n",
        "plt.plot(x, poly1d_fn(x), '--k')\n",
        "\n",
        "plt.xlabel(\"Accuracy\", fontsize=12)\n",
        "plt.ylabel(\"Explainability\", fontsize=12)\n",
        "plt.xticks(fontsize=12)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}