{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "name": "#1_Preprocessing_Data.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "jcpVFx38F_v_",
        "dz1i3gefbF1e",
        "dtOGTFribF1g",
        "tDQO4v-cbF1h"
      ]
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "105dbb5bcc264df3a6fdee98d00e823c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_52e862f7892c4384b7a4741c63380628",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_d2cfb08b747e40af9321fc0b070246e2",
              "IPY_MODEL_2f6d73027f9d4c65bdf411821d211d5b"
            ]
          }
        },
        "52e862f7892c4384b7a4741c63380628": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d2cfb08b747e40af9321fc0b070246e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_c66fe09656c749c783cd451e3800e5cb",
            "_dom_classes": [],
            "description": "Text Processing: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 20148,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 20148,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_61a91e4837d34346ad86ed24d7076ad5"
          }
        },
        "2f6d73027f9d4c65bdf411821d211d5b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_c25308a986a24b1ba57044912e99861b",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 20148/20148 [02:17&lt;00:00, 146.34it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_71f1451baa2c4114b4da0669cae03f1b"
          }
        },
        "c66fe09656c749c783cd451e3800e5cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "61a91e4837d34346ad86ed24d7076ad5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c25308a986a24b1ba57044912e99861b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "71f1451baa2c4114b4da0669cae03f1b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GfYvsMq9bF1d"
      },
      "source": [
        "# Notebook 1: Preprocessing of data\n",
        "\n",
        "This notebook presents the preprocessing of the data as explained in the paper in section 4.2. By doing so, the posts are prepared for subsequent feature extraction while the rationales and labels are aggregated.\n",
        "\n",
        "\n",
        "**Table of Contents**:\n",
        "0. [Technical setup](#setup)\n",
        "1. [Load documents and aggregate labels/rationales](#load)\n",
        "2. [Preprocess posts](#preprocessing)\n",
        "3. [Export preprocessed dataframe](#export)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jcpVFx38F_v_"
      },
      "source": [
        "# 1.0 Technical setup <a id=\"setup\"></a>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IDUkhaE3bu3H",
        "outputId": "f3723f51-231d-46b7-c648-f9b27b7370ab"
      },
      "source": [
        "# set up Google Colab workspace\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!pip install emoji"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fDkZDvsibF1d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03019ff5-7a00-448b-828a-9b5fae1633d8"
      },
      "source": [
        "# loading modules\n",
        "\n",
        "import pandas as pd\n",
        "pd.set_option('display.max_colwidth', 300)\n",
        "import re\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.notebook import tqdm\n",
        "from collections import Counter\n",
        "import nltk\n",
        "from nltk.corpus import stopwords, words, wordnet\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk import WordNetLemmatizer, pos_tag, word_tokenize\n",
        "from nltk.corpus.reader.wordnet import WordNetError\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('words')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "from sklearn import preprocessing\n",
        "import pickle\n",
        "from copy import deepcopy\n",
        "import json\n",
        "import emoji\n",
        "import itertools"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xtbg2zPfbF1e"
      },
      "source": [
        "# define functions for saving and loading pickled objects\n",
        "def save_pickle(objectname, picklename):\n",
        "    pickle_out = open(picklename,\"wb\")\n",
        "    pickle.dump(objectname, pickle_out)\n",
        "    pickle_out.close()\n",
        "    print(picklename, 'successfully pickled.')\n",
        "\n",
        "def load_pickle(picklename):\n",
        "    pickle_in = open(picklename,\"rb\")\n",
        "    return pickle.load(pickle_in)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dz1i3gefbF1e"
      },
      "source": [
        "# 1.1 Load documents and aggregate labels/rationales <a id=\"load\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R1zPtG6BbF1e"
      },
      "source": [
        "# import dataset\n",
        "\n",
        "with open('/content/drive/MyDrive/Seminar/dataset.txt') as json_file:\n",
        "    hateXplain_raw = json.load(json_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CEabpAOCbF1e"
      },
      "source": [
        "# create binary classification problem by majority voting (0: normal, 1: hateful)\n",
        "def gen_label(dict_lab):\n",
        "    n = len(dict_lab)\n",
        "    labels = [dict_lab[i]['label'] for i in range(n)]\n",
        "    c = Counter(labels)\n",
        "    label_mostfreq = c.most_common(1)\n",
        "    if label_mostfreq[0][0] == 'normal':\n",
        "        l = 0\n",
        "    else:\n",
        "        l = 1\n",
        "    return l\n",
        "\n",
        "# combine rationales by adding the ones of all annotators\n",
        "def gen_rational(list_rat):\n",
        "    if len(list_rat) == 0:\n",
        "        return []\n",
        "    \n",
        "    rationales = [0] * len(list_rat[0])\n",
        "    for rat in list_rat:\n",
        "        rationales = [sum(x) for x in zip(rationales, rat)]\n",
        "    rationales = [1 if i > 1 else i for i in rationales]\n",
        "    return rationales"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kahrYoXGbF1f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "outputId": "3c63d87b-af7d-4e4a-8e4c-5b1cc01ca211"
      },
      "source": [
        "# create dataframe of data\n",
        "\n",
        "hateXplain_df = pd.DataFrame.transpose(pd.DataFrame.from_records(hateXplain_raw)).drop(columns=['post_id'])\n",
        "hateXplain_df['label'] = [gen_label(hateXplain_df[\"annotators\"][i]) for i in range(hateXplain_df.shape[0])]\n",
        "hateXplain_df['rationales_comb'] = [gen_rational(hateXplain_df[\"rationales\"][i]) for i in range(hateXplain_df.shape[0])]\n",
        "\n",
        "hateXplain_df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>annotators</th>\n",
              "      <th>rationales</th>\n",
              "      <th>post_tokens</th>\n",
              "      <th>label</th>\n",
              "      <th>rationales_comb</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>10001291_gab</th>\n",
              "      <td>[{'label': 'hatespeech', 'annotator_id': 199, 'target': ['Jewish']}, {'label': 'hatespeech', 'annotator_id': 200, 'target': ['Jewish']}, {'label': 'hatespeech', 'annotator_id': 201, 'target': ['Jewish']}]</td>\n",
              "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]</td>\n",
              "      <td>[jews, are, responsible, for, destroying, my, towns, culture, and, turning, leeds, into, a, ghetto, hell, of, dollarstan, full, of, dollarstanis, please, killalljewz]</td>\n",
              "      <td>1</td>\n",
              "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10002346_gab</th>\n",
              "      <td>[{'label': 'normal', 'annotator_id': 202, 'target': ['Men', 'Other']}, {'label': 'offensive', 'annotator_id': 203, 'target': ['Other']}, {'label': 'normal', 'annotator_id': 204, 'target': ['Women']}]</td>\n",
              "      <td>[]</td>\n",
              "      <td>[leftist, mind, control, fails, i, think, this, overstates, it, men, prefer, women, who, are, real, people, and, prefer, tall, thin, young, bodies, if, asked, in, abstraction, the, two, are, not, the, same, question]</td>\n",
              "      <td>0</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10007406_gab</th>\n",
              "      <td>[{'label': 'offensive', 'annotator_id': 205, 'target': ['Homosexual']}, {'label': 'hatespeech', 'annotator_id': 206, 'target': ['Homosexual']}, {'label': 'normal', 'annotator_id': 207, 'target': ['Homosexual']}]</td>\n",
              "      <td>[]</td>\n",
              "      <td>[the, scientific, findings, that, advocates, and, the, pro, gay, media, cite, tend, to, come, from, decidedly, unrepresentative, samples, that, have, been, collected, from, places, like, bulletin, boards, at, gay, parenting, support, groups, where, everyone, has, an, interest, in, proving, that,...</td>\n",
              "      <td>1</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10008196_gab</th>\n",
              "      <td>[{'label': 'hatespeech', 'annotator_id': 200, 'target': ['African']}, {'label': 'hatespeech', 'annotator_id': 208, 'target': ['African']}, {'label': 'normal', 'annotator_id': 209, 'target': ['None']}]</td>\n",
              "      <td>[[0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...</td>\n",
              "      <td>[it, why, they, behave, like, animals, no, matter, where, you, put, them, they, will, destroy, anything, of, value, black, people, want, to, claim, we, are, the, racists, but, then, why, do, they, move, out, of, the, ghetto, once, they, have, success, even, they, do, not, want, to, live, around,...</td>\n",
              "      <td>1</td>\n",
              "      <td>[0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10009556_gab</th>\n",
              "      <td>[{'label': 'offensive', 'annotator_id': 27, 'target': ['Asian']}, {'label': 'offensive', 'annotator_id': 1, 'target': ['Arab']}, {'label': 'offensive', 'annotator_id': 52, 'target': ['Arab']}]</td>\n",
              "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]</td>\n",
              "      <td>[why, are, people, so, offended, by, the, word, paki, surely, the, word, fucking, that, comes, before, is, more, offensive]</td>\n",
              "      <td>1</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                                                                                                                       annotators  ...                                                                                                                                               rationales_comb\n",
              "10001291_gab         [{'label': 'hatespeech', 'annotator_id': 199, 'target': ['Jewish']}, {'label': 'hatespeech', 'annotator_id': 200, 'target': ['Jewish']}, {'label': 'hatespeech', 'annotator_id': 201, 'target': ['Jewish']}]  ...                                                                                            [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
              "10002346_gab              [{'label': 'normal', 'annotator_id': 202, 'target': ['Men', 'Other']}, {'label': 'offensive', 'annotator_id': 203, 'target': ['Other']}, {'label': 'normal', 'annotator_id': 204, 'target': ['Women']}]  ...                                                                                                                                                            []\n",
              "10007406_gab  [{'label': 'offensive', 'annotator_id': 205, 'target': ['Homosexual']}, {'label': 'hatespeech', 'annotator_id': 206, 'target': ['Homosexual']}, {'label': 'normal', 'annotator_id': 207, 'target': ['Homosexual']}]  ...                                                                                                                                                            []\n",
              "10008196_gab             [{'label': 'hatespeech', 'annotator_id': 200, 'target': ['African']}, {'label': 'hatespeech', 'annotator_id': 208, 'target': ['African']}, {'label': 'normal', 'annotator_id': 209, 'target': ['None']}]  ...  [0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
              "10009556_gab                     [{'label': 'offensive', 'annotator_id': 27, 'target': ['Asian']}, {'label': 'offensive', 'annotator_id': 1, 'target': ['Arab']}, {'label': 'offensive', 'annotator_id': 52, 'target': ['Arab']}]  ...                                                                                                     [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dtOGTFribF1g"
      },
      "source": [
        "# 1.2 Preprocess posts <a id=\"preprocessing\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cOdIgGvObF1g"
      },
      "source": [
        "# Preprocessing to return the cleaned text as a list of words as string\n",
        "\n",
        "lem = WordNetLemmatizer() #load lemmatizer\n",
        "eng_vocab = set(words.words()) # load Englisch vocabulary\n",
        "special_characters = '!@#$%^&*()-+?_=,<>/'\n",
        "\n",
        "def lemmatize(word): #function to lemmatize a word\n",
        "    pos_label = (nltk.pos_tag([word])[0][1][0]).lower() #identify single character pos-constant from pos_tag\n",
        "    \n",
        "    if pos_label == 'j': pos_label = 'a'    # 'j' <-> 'a' reassignment for adjectives because 'j' is not in wordnet: 'a' as label for adjectives\n",
        "    \n",
        "    if pos_label in ['r']:  # identify and lemmatize adverbs \n",
        "        try:\n",
        "            return wordnet.synset(word+'.r.1').lemmas()[0].pertainyms()[0].name()\n",
        "        except (IndexError, WordNetError):\n",
        "            return word\n",
        "    elif pos_label in ['a', 's', 'v']: # identify and lemmatize (satellite) adjectives and verbs\n",
        "        return lem.lemmatize(word, pos=pos_label)\n",
        "    else:   # lemmatize nouns and everything else\n",
        "        return lem.lemmatize(word)\n",
        "    \n",
        "def text_process(text):\n",
        "    text_processed =  [word for word in text if \n",
        "                       (word not in stopwords.words('english') and  # remove stopwords\n",
        "                        not word.isdigit() and # remove digits\n",
        "                        not any(c in special_characters for c in word) and # remove words with special characters\n",
        "                        len(re.findall(r'[^\\w\\s,.]', word)) == 0 and # remove emojis and other special sequences\n",
        "                        len(word) > 1 and # remove empty strings\n",
        "                        word.isascii())] # remove non-English characters\n",
        "    \n",
        "    rem_words = [word for word in text if word not in text_processed] # find words which were removed\n",
        "    ind_rem_words = []\n",
        "    for word in rem_words:\n",
        "        ind_rem_words.append([i for i, x in enumerate(text) if x == word])\n",
        "    ind_rem_words = list(set(itertools.chain.from_iterable(ind_rem_words))) # get indices of removed words\n",
        "    \n",
        "    text_processed = [word.lower() for word in text_processed]\n",
        "    text_lemmatized = [lemmatize(word) for word in text_processed] #lemmatize words\n",
        "    return text_lemmatized, ind_rem_words\n",
        "\n",
        "# adapt rationales by removing indices of the words which were removed\n",
        "# during the preprocessing\n",
        "def new_rationale(old_ind, ind_remove):\n",
        "    if len(old_ind) < 2:\n",
        "        return []\n",
        "    return [i for j, i in enumerate(old_ind) if j not in ind_remove]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OtsqpvZZbF1g",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "105dbb5bcc264df3a6fdee98d00e823c",
            "52e862f7892c4384b7a4741c63380628",
            "d2cfb08b747e40af9321fc0b070246e2",
            "2f6d73027f9d4c65bdf411821d211d5b",
            "c66fe09656c749c783cd451e3800e5cb",
            "61a91e4837d34346ad86ed24d7076ad5",
            "c25308a986a24b1ba57044912e99861b",
            "71f1451baa2c4114b4da0669cae03f1b"
          ]
        },
        "outputId": "2bbf8e1e-c0df-4f15-da5f-9fc55ad611a4"
      },
      "source": [
        "# Preprocessing of texts\n",
        "\n",
        "processed_text = []\n",
        "rationales_adapted = []\n",
        "for ind in tqdm(range(hateXplain_df.shape[0]), \"Text Processing\"):\n",
        "    processed_sentence, ind_remove = text_process(hateXplain_df['post_tokens'][ind])\n",
        "    processed_text.append(processed_sentence)\n",
        "    rationales_adapted.append(new_rationale(hateXplain_df['rationales_comb'][ind], ind_remove))            \n",
        "\n",
        "hateXplain_df['tokens_processed'] = processed_text\n",
        "hateXplain_df['rationales_comb'] = rationales_adapted"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "105dbb5bcc264df3a6fdee98d00e823c",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Text Processing', max=20148.0, style=ProgressStyle(descriâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "85TB7uA3bF1h",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        },
        "outputId": "691e455b-a646-4ed1-e569-5ca559c81998"
      },
      "source": [
        "hateXplain_df = hateXplain_df.reset_index(drop=True)\n",
        "\n",
        "# remove empty rows\n",
        "ind_empty = [i for i in range(len(hateXplain_df['tokens_processed'])) if len(hateXplain_df['tokens_processed'][i]) == 0]\n",
        "hateXplain_df.drop(ind_empty, inplace = True)\n",
        "\n",
        "hateXplain_df = hateXplain_df.reset_index(drop=True)\n",
        "hateXplain_df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>annotators</th>\n",
              "      <th>rationales</th>\n",
              "      <th>post_tokens</th>\n",
              "      <th>label</th>\n",
              "      <th>rationales_comb</th>\n",
              "      <th>tokens_processed</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[{'label': 'hatespeech', 'annotator_id': 199, 'target': ['Jewish']}, {'label': 'hatespeech', 'annotator_id': 200, 'target': ['Jewish']}, {'label': 'hatespeech', 'annotator_id': 201, 'target': ['Jewish']}]</td>\n",
              "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]</td>\n",
              "      <td>[jews, are, responsible, for, destroying, my, towns, culture, and, turning, leeds, into, a, ghetto, hell, of, dollarstan, full, of, dollarstanis, please, killalljewz]</td>\n",
              "      <td>1</td>\n",
              "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]</td>\n",
              "      <td>[jew, responsible, destroy, town, culture, turn, leeds, ghetto, hell, dollarstan, full, dollarstanis, please, killalljewz]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[{'label': 'normal', 'annotator_id': 202, 'target': ['Men', 'Other']}, {'label': 'offensive', 'annotator_id': 203, 'target': ['Other']}, {'label': 'normal', 'annotator_id': 204, 'target': ['Women']}]</td>\n",
              "      <td>[]</td>\n",
              "      <td>[leftist, mind, control, fails, i, think, this, overstates, it, men, prefer, women, who, are, real, people, and, prefer, tall, thin, young, bodies, if, asked, in, abstraction, the, two, are, not, the, same, question]</td>\n",
              "      <td>0</td>\n",
              "      <td>[]</td>\n",
              "      <td>[leftist, mind, control, fails, think, overstates, men, prefer, woman, real, people, prefer, tall, thin, young, body, ask, abstraction, two, question]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[{'label': 'offensive', 'annotator_id': 205, 'target': ['Homosexual']}, {'label': 'hatespeech', 'annotator_id': 206, 'target': ['Homosexual']}, {'label': 'normal', 'annotator_id': 207, 'target': ['Homosexual']}]</td>\n",
              "      <td>[]</td>\n",
              "      <td>[the, scientific, findings, that, advocates, and, the, pro, gay, media, cite, tend, to, come, from, decidedly, unrepresentative, samples, that, have, been, collected, from, places, like, bulletin, boards, at, gay, parenting, support, groups, where, everyone, has, an, interest, in, proving, that,...</td>\n",
              "      <td>1</td>\n",
              "      <td>[]</td>\n",
              "      <td>[scientific, finding, advocate, pro, gay, medium, cite, tend, come, decided, unrepresentative, sample, collect, place, like, bulletin, board, gay, parent, support, group, everyone, interest, prove, gay, parent, success, austin, ruse]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[{'label': 'hatespeech', 'annotator_id': 200, 'target': ['African']}, {'label': 'hatespeech', 'annotator_id': 208, 'target': ['African']}, {'label': 'normal', 'annotator_id': 209, 'target': ['None']}]</td>\n",
              "      <td>[[0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...</td>\n",
              "      <td>[it, why, they, behave, like, animals, no, matter, where, you, put, them, they, will, destroy, anything, of, value, black, people, want, to, claim, we, are, the, racists, but, then, why, do, they, move, out, of, the, ghetto, once, they, have, success, even, they, do, not, want, to, live, around,...</td>\n",
              "      <td>1</td>\n",
              "      <td>[1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
              "      <td>[behave, like, animal, matter, put, destroy, anything, value, black, people, want, claim, racist, move, ghetto, success, even, want, live, around, kind]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[{'label': 'offensive', 'annotator_id': 27, 'target': ['Asian']}, {'label': 'offensive', 'annotator_id': 1, 'target': ['Arab']}, {'label': 'offensive', 'annotator_id': 52, 'target': ['Arab']}]</td>\n",
              "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]</td>\n",
              "      <td>[why, are, people, so, offended, by, the, word, paki, surely, the, word, fucking, that, comes, before, is, more, offensive]</td>\n",
              "      <td>1</td>\n",
              "      <td>[0, 0, 0, 1, 0, 0, 0, 0, 0]</td>\n",
              "      <td>[people, offend, word, paki, sure, word, fuck, come, offensive]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                                                                                                            annotators  ...                                                                                                                                                                                                                           tokens_processed\n",
              "0         [{'label': 'hatespeech', 'annotator_id': 199, 'target': ['Jewish']}, {'label': 'hatespeech', 'annotator_id': 200, 'target': ['Jewish']}, {'label': 'hatespeech', 'annotator_id': 201, 'target': ['Jewish']}]  ...                                                                                                                 [jew, responsible, destroy, town, culture, turn, leeds, ghetto, hell, dollarstan, full, dollarstanis, please, killalljewz]\n",
              "1              [{'label': 'normal', 'annotator_id': 202, 'target': ['Men', 'Other']}, {'label': 'offensive', 'annotator_id': 203, 'target': ['Other']}, {'label': 'normal', 'annotator_id': 204, 'target': ['Women']}]  ...                                                                                     [leftist, mind, control, fails, think, overstates, men, prefer, woman, real, people, prefer, tall, thin, young, body, ask, abstraction, two, question]\n",
              "2  [{'label': 'offensive', 'annotator_id': 205, 'target': ['Homosexual']}, {'label': 'hatespeech', 'annotator_id': 206, 'target': ['Homosexual']}, {'label': 'normal', 'annotator_id': 207, 'target': ['Homosexual']}]  ...  [scientific, finding, advocate, pro, gay, medium, cite, tend, come, decided, unrepresentative, sample, collect, place, like, bulletin, board, gay, parent, support, group, everyone, interest, prove, gay, parent, success, austin, ruse]\n",
              "3             [{'label': 'hatespeech', 'annotator_id': 200, 'target': ['African']}, {'label': 'hatespeech', 'annotator_id': 208, 'target': ['African']}, {'label': 'normal', 'annotator_id': 209, 'target': ['None']}]  ...                                                                                   [behave, like, animal, matter, put, destroy, anything, value, black, people, want, claim, racist, move, ghetto, success, even, want, live, around, kind]\n",
              "4                     [{'label': 'offensive', 'annotator_id': 27, 'target': ['Asian']}, {'label': 'offensive', 'annotator_id': 1, 'target': ['Arab']}, {'label': 'offensive', 'annotator_id': 52, 'target': ['Arab']}]  ...                                                                                                                                                                            [people, offend, word, paki, sure, word, fuck, come, offensive]\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tDQO4v-cbF1h"
      },
      "source": [
        "# 1.3 Export preprocessed dataframe <a id=\"export\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N07DmHf0bF1h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7110c489-fb97-404d-fa5b-6f9ef4d6a7d0"
      },
      "source": [
        "# export dataset as pickle\n",
        "save_pickle(hateXplain_df, '/content/drive/MyDrive/Seminar/hateXplain_processed.pickle')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/Seminar/hateXplain_processed.pickle successfully pickled.\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}